# ============================================
# DSPy Text Classification Studio
# Environment Configuration
# ============================================
# Copy this file to .env and fill in your values:
#   cp .env.example .env
# ============================================

# ============================================
# ü§ñ AI PROVIDER SELECTION
# ============================================
# Choose one of: ollama, gemini, openai, huggingface
#
# ollama     - FREE, runs locally, no rate limits (RECOMMENDED)
# gemini     - Google's Gemini API (free tier has limits)
# openai     - OpenAI API (paid)
# huggingface - HuggingFace Inference API (free tier available)

PROVIDER=ollama

# ============================================
# üè† OLLAMA SETTINGS (FREE - Local AI)
# ============================================
# Install: brew install ollama
# Start:   brew services start ollama
# Models:  ollama pull phi3:mini
#
# Recommended models for Intel Mac:
#   phi3:mini   - 2.2 GB, fast, good quality
#   llama3.2    - 4.7 GB, balanced
#   mistral     - 4.1 GB, general purpose

OLLAMA_MODEL=phi3:mini
OLLAMA_BASE_URL=http://localhost:11434

# ============================================
# üåê GOOGLE GEMINI SETTINGS
# ============================================
# Get API key: https://aistudio.google.com/app/apikey
# Note: Free tier has rate limits (may hit 429 errors)

GOOGLE_API_KEY=
GOOGLE_MODEL=gemini-2.0-flash

# ============================================
# üîë OPENAI SETTINGS
# ============================================
# Get API key: https://platform.openai.com/api-keys
# Note: Paid API, charges per token

OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini

# ============================================
# ü§ó HUGGINGFACE SETTINGS
# ============================================
# Get token: https://huggingface.co/settings/tokens
# Note: Free tier available with some limits

HF_TOKEN=
HF_MODEL=mistralai/Mistral-7B-Instruct-v0.3

# ============================================
# üñ•Ô∏è SERVER SETTINGS
# ============================================
# Host: 0.0.0.0 for all interfaces, 127.0.0.1 for localhost only
# Port: Default is 8080

HOST=0.0.0.0
PORT=8080
DEBUG=false
LOG_LEVEL=INFO
